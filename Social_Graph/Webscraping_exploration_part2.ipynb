{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In part one we obtained photo captions by scraping a socialite blog/website\n",
    "# Here we extract the names of people from the captions\n",
    "\n",
    "# This project will scrape a New York Social Diary website and identify divs with\n",
    "# photos and photo captions. Parsing the captions will yield a social network graph \n",
    "# of the New York Social Elite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import re\n",
    "from xml.sax.saxutils import escape\n",
    "from HTMLParser import HTMLParser\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import dill\n",
    "import time\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import csv\n",
    "#import nltk\n",
    "#from nltk.tag import pos_tag\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findword(item):\n",
    "    try:\n",
    "        \n",
    "            if (len(str(item))<250):\n",
    "                \n",
    "                item=str(item)\n",
    "                item= item.replace(\" von\",\" Von\") # try  \" von \" and \" van \"\n",
    "                item= item.replace(\" van\",\" Van\")\n",
    "                item= item.replace(\" de\",\" De\") # changes Frederick to Fre Derick...\n",
    "                sentense= str(re.findall(r'''([A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+)''',item))\n",
    "                sentense = sentense.replace(\"Council \",\"\")\n",
    "                sentense = sentense.replace(\"Councilman \",\"\")\n",
    "                sentense = sentense.replace(\"Councilmember \",\"\")\n",
    "                sentense = sentense.replace(\"Councilwoman \",\"\")\n",
    "                sentense = sentense.replace(\"Congresswoman \",\"\")\n",
    "                sentense = sentense.replace(\"New York\",\"\")\n",
    "                sentense = sentense.replace(\"Board\",\"\")\n",
    "                sentense = sentense.replace(\"Member\",\"\")\n",
    "                sentense = sentense.replace(\"Congressman \",\"\")\n",
    "                sentense = sentense.replace(\"Opening Gala\",\"\")\n",
    "                sentense = sentense.replace(\"History\",\"\")\n",
    "                sentense = sentense.replace(\"Museum\",\"\")\n",
    "                sentense = sentense.replace(\"American\",\"\")\n",
    "                sentense = sentense.replace(\"Congressman \",\"\")\n",
    "                sentense = sentense.replace(\"Night \",\"\")\n",
    "                sentense = sentense.replace(\"Miss \",\"\")\n",
    "                sentense = sentense.replace(\"USA \",\"\")         \n",
    "                sentense = sentense.replace(\"Mayor \",\"\")\n",
    "                sentense = sentense.replace(\"Event Co\",\"\") # ???\n",
    "                sentense = sentense.replace(\"Count \",\"\")\n",
    "                sentense = sentense.replace(\"Countess \",\"\")\n",
    "                sentense = sentense.replace(\"Chairman \",\"\")\n",
    "                sentense = sentense.replace(\"Chairwoman \",\"\")\n",
    "                sentense = sentense.replace(\"President \",\"\")\n",
    "                sentense = sentense.replace(\"Museum \",\"\")\n",
    "                sentense = sentense.replace(\"Chairmen \",\"\")\n",
    "                sentense = sentense.replace(\"DJ \",\"\")\n",
    "                sentense = sentense.replace(\"MD \",\"\")\n",
    "                sentense = sentense.replace(\"Dance \",\"\")\n",
    "                sentense = sentense.replace(\"Director \",\"\")\n",
    "                sentense = sentense.replace(\"Dr. \",\"\")\n",
    "                sentense = sentense.replace(\"Sir \",\"\")\n",
    "                sentense = sentense.replace(\"The \",\"\")\n",
    "                sentense = sentense.replace(\"Society\",\"\")\n",
    "                sentense = sentense.replace(\"Memorial Sloan\",\"\")                \n",
    "                sentense = sentense.replace(\"Jr. \",\"\")\n",
    "                sentense = sentense.replace(\"CEO \",\"\")\n",
    "                sentense = sentense.replace(\"[\", \"\")    # these ones ???\n",
    "                sentense = sentense.replace(\"]\", \"\")\n",
    "                sentense = sentense.replace(\"' \", \"'\")\n",
    "                sentense = sentense.replace(\" ' \", \"'\")\n",
    "                sentense = sentense.replace(\" '\", \"'\")\n",
    "                sentense = sentense.replace(\"'\", \"\")\n",
    "                sentense = sentense.replace(\"Executive Director\",\"\")\n",
    "                sentense = sentense.replace(\"Presi\",\"\") # not needed\n",
    "                sentense = sentense.replace(\"Co \",\"\")\n",
    "                sentense = sentense.replace(\"Steering Committee \",\"\")\n",
    "                sentense = sentense.replace(\"Vice Presi \",\"\") # not needed ?\n",
    "                sentense = sentense.replace(\"Legal Defense\",\"\")\n",
    "                sentense = sentense.replace(\"Educational Fund\",\"\")\n",
    "                sentense = sentense.replace(\"National Equal Justice Award Dinner\",\"\")\n",
    "                sentense = sentense.replace(\"Annual Gala\",\"\")\n",
    "                #['Committee  Katie Boes']\n",
    "                #['Dinosaur Fossil Table']\n",
    "                #['Zoological Conservation Center']\n",
    "                #['Magician Looney Louie']\n",
    "                #['Science Center']\n",
    "                #['Science Center']\n",
    "                #['Three Little Operation Smile Bears']\n",
    "                #['Brooklyn Food Coalition Kady Ferguson']\n",
    "                #['Ross Award Winner David Schwarz']\n",
    "                #['Ross Award Winner Edward Fraughton']\n",
    "                #['Sister Jane Gerety']\n",
    "                #['Holiday House Hamptons Designers']\n",
    "                #['Frederick Law Olmsted Awards Luncheon']\n",
    "                #['Principal Dancer Ashley Bouder']\n",
    "                #['Soloist Justin Peck']\n",
    "                #['Mount Sinai Crystal Party']\n",
    "                #['Central Park Conservatory Garden']\n",
    "                # errors to be added ... ['Speaker Melissa Mark']\n",
    "                #['Family Dynamics Art Auction'] \n",
    "                #['Taittinger Nocturne Champagne']\n",
    "                #['Friars Club Dean Emeritus Freddie Roman']\n",
    "                return list(sentense.split(','))\n",
    "           \n",
    "            \n",
    "    except Exception, f:\n",
    "        y=f\n",
    "        print \"Hell broke loose in get nltp area!\"+str(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_words=[]    \n",
    "f = open('NOV16_az_captions_full.txt', 'r') # test_az_captions.txt\n",
    "for line in iter (f):\n",
    "    w=findword(line)\n",
    "    if str(w) == str(['']):\n",
    "        pass\n",
    "    else:\n",
    "        list_of_words.append(w)\n",
    "f.close()\n",
    "list_of_words = [x for x in list_of_words if x != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bart Scott', 'Mark Laplander']\n"
     ]
    }
   ],
   "source": [
    "print list_of_words[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('new_filename_NOV16_full.txt', 'w') as f:\n",
    "    for s in list_of_words:\n",
    "        f.write(str(s) + '\\n')\n",
    "        \n",
    "# save to pickle\n",
    "with open(\"all_names_for_graph_full.pickle\", \"wb\") as file:\n",
    "    pickle.dump(list_of_words, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Les Lieberman',\n",
       "  'Barri Lieberman',\n",
       "  'Isabel Kallman',\n",
       "  'Trish Iervolino',\n",
       "  'Ron Iervolino'],\n",
       " ['Chuck Grodin']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_words[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# check for empty strings from regex ?\n",
    "sumss = 0\n",
    "for line in list_of_words:\n",
    "    if str(line) == str(['']):\n",
    "        #print line,str(line)\n",
    "        sumss +=1\n",
    "print sumss\n",
    "#str(data[18]) == str([''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there were 30674 single names\n"
     ]
    }
   ],
   "source": [
    "data_combo=[]\n",
    "data=[]\n",
    "sumss  = 0\n",
    "\n",
    "for line in list_of_words:\n",
    "    if line is None:\n",
    "        pass\n",
    "    elif len(line) == 1:\n",
    "        sumss += 1\n",
    "    else:\n",
    "        data.append(line)  \n",
    "        #print line\n",
    "\n",
    "print \"there were \"+ str(sumss) + \" single names\" # these are not needed and are dropped later in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('new_filename_NOV16_full_doubleormore.txt', 'w') as f:\n",
    "    for s in data:\n",
    "        f.write(str(s) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67685, 98361, -30676)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data),len(list_of_words),len(data)-len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In part 3 we will form a social network graph and calculate best friends, page rank and most popular people\n",
    "# In part 4 we explore visualization tools such as graph_tool and Gephi for social graph networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
